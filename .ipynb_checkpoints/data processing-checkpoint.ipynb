{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "import re\n",
    "import os.path\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict, OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jinshin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "# Initialize Stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Word match regex to exclude digits\n",
    "word_pattern = re.compile(r'[^\\W\\d]')\n",
    "# Set of stopwords\n",
    "stopword_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed text files already present.\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('uci-news-aggregator.csv')\n",
    "\n",
    "# List containing each title as item\n",
    "titles = [title for title in dataset['TITLE']]\n",
    "true_label = [label for label in dataset['CATEGORY']]\n",
    "\n",
    "# Only create stemmed word file if no file exists\n",
    "if (not os.path.exists('stemmed.txt') or not os.path.exists('stemmed_sentences.txt')):\n",
    "    print(\"Creating text files to extract from......\")\n",
    "    with open('stemmed.txt', 'w') as file, open('stemmed_sentences.txt', 'w') as file1:\n",
    "        for title in titles:\n",
    "            temp_title = []\n",
    "            # split title into vector of words\n",
    "            for word in title.split():\n",
    "                word = word.lower().rstrip('?:!.,;')\n",
    "                if word not in stopword_set and 'http://' not in word and 'www' not in word:\n",
    "                    if word_pattern.match(word):\n",
    "                        # Add stemmed words to text file\n",
    "                        word_temp = ps.stem(word).rstrip(\"'\")\n",
    "                        file.write(word_temp + '\\n')\n",
    "                        temp_title.append(word_temp)\n",
    "            # Write titles to stemmed_sentences.txt\n",
    "            file1.write(' '.join(temp_title) + '\\n')\n",
    "else:\n",
    "    print (\"stemmed text files already present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Creating bag of words\n"
     ]
    }
   ],
   "source": [
    "# List of all stemmed words (bag of words)\n",
    "all_words = []\n",
    "corpus_freq = defaultdict(int)\n",
    "feature = OrderedDict()\n",
    "with open('stemmed.txt', 'r') as file:\n",
    "    for word in file:\n",
    "        word = word.strip('\\n')\n",
    "        all_words.append(word)\n",
    "        corpus_freq[word] += 1.0\n",
    "        feature[word] = 0\n",
    "titles_doc_vector = []\n",
    "with open('stemmed_sentences.txt', 'r') as file1:\n",
    "    for title in file1:\n",
    "        new_doc_vec = defaultdict(int)\n",
    "        for word in title.split():\n",
    "            new_doc_vec[word] += 1\n",
    "        titles_doc_vector.append(new_doc_vec)\n",
    "print(\"Finished Creating bag of words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with freq = 1: 24982\n",
      "Number of words with freq = 2: 6598\n",
      "Number of words with freq = 3: 3313\n",
      "Number of words with freq = 4: 2066\n",
      "Number of words with freq = 5: 1419\n",
      "Number of words with freq = 6: 1096\n",
      "Number of words with freq = 7: 898\n",
      "Number of words with freq = 8: 724\n",
      "Number of words with freq = 9: 620\n",
      "Number of words with freq = 10: 505\n",
      "\n",
      "\n",
      "finished processing\n",
      "length of features is 12239\n",
      "length of features is 12239\n",
      "length of examples is 422419\n"
     ]
    }
   ],
   "source": [
    "freq_count = defaultdict(int)\n",
    "for key, val in corpus_freq.items():\n",
    "    freq_count[val] += 1\n",
    "print(\"Number of words with freq = 1: \" + str(freq_count[1]))\n",
    "print(\"Number of words with freq = 2: \" + str(freq_count[2]))\n",
    "print(\"Number of words with freq = 3: \" + str(freq_count[3]))\n",
    "print(\"Number of words with freq = 4: \" + str(freq_count[4]))\n",
    "print(\"Number of words with freq = 5: \" + str(freq_count[5]))\n",
    "print(\"Number of words with freq = 6: \" + str(freq_count[6]))\n",
    "print(\"Number of words with freq = 7: \" + str(freq_count[7]))\n",
    "print(\"Number of words with freq = 8: \" + str(freq_count[8]))\n",
    "print(\"Number of words with freq = 9: \" + str(freq_count[9]))\n",
    "print(\"Number of words with freq = 10: \" + str(freq_count[10]))\n",
    "\n",
    "#removing unnecessary\n",
    "for key, val in corpus_freq.items():\n",
    "    if val >= 0 and val <= 10:\n",
    "        del corpus_freq[key]\n",
    "        del feature[key]\n",
    "print('\\n')\n",
    "print('finished processing')\n",
    "print(\"length of features is \" +str(len(feature)))\n",
    "print(\"length of features is \" +str(len(corpus_freq)))\n",
    "print(\"length of examples is \" + str(len(titles_doc_vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = OrderedDict()\n",
    "counter = 0\n",
    "json_counter = 0\n",
    "for freq_dict in titles_doc_vector:\n",
    "    counter += 1\n",
    "    instance = feature.copy()\n",
    "    for word, freq in freq_dict.items():\n",
    "        if word in corpus_freq and corpus_freq[word] != 0:\n",
    "            instance[word] = freq / corpus_freq[word]\n",
    "    feature_vector[counter] = instance.values()\n",
    "    if counter == 1000:\n",
    "        json_counter += 1\n",
    "        with open('feature_json/feature_vector' + str(json_counter) + '.json', 'w') as fp:\n",
    "            json.dump(feature_vector, fp)\n",
    "            feature_vector.clear()\n",
    "            print('finished ' + str(json_counter) + ' json file')\n",
    "            counter = 0\n",
    "\n",
    "with open('feature_vector' + str(json_counter) + '.json', 'w') as fp:\n",
    "    json.dump(feature_vector, fp)\n",
    "    feature_vector.clear()\n",
    "    print('finished ' + str(json_counter) + ' json file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completely loaded data\n"
     ]
    }
   ],
   "source": [
    "#load json file\n",
    "data_vector = []\n",
    "for i in range(1, 21):\n",
    "    data_json = json.load(open('feature_json/feature_vector' + str(i) + '.json'), object_pairs_hook=OrderedDict)\n",
    "    data_vector += data_json.values()\n",
    "print(\"Completely loaded data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, train_labels, test_labels = train_test_split(data_vector,\n",
    "                                                      true_label[:20000],\n",
    "                                                      test_size=0.33)\n",
    "#using Decision Tree Classifier to see performance\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(train, train_labels)\n",
    "preds = model.predict(test)\n",
    "print ('%s %d %s %.3f %s %s %d %s %.3f %s' % (\"### DECISION TREE OVERALL CORRECT: \", \n",
    "accuracy_score(test_labels, preds) * len(test_labels), \" = \",  \n",
    "accuracy_score(test_labels, preds) * 100, \"%   \", \"INCORRECT: \", \n",
    "len(test_labels) - accuracy_score(test_labels, preds)*len(test_labels), \" = \",  \n",
    "100 - accuracy_score(test_labels, preds) * 100, \"%\")) #print out results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Naive Bayes\n",
    "model = SVC()\n",
    "y_pred = model.fit(train, train_labels).predict(test)\n",
    "print ('%s %d %s %.3f %s %s %d %s %.3f %s' % (\"### Naive Bayes OVERALL CORRECT: \", \n",
    "accuracy_score(test_labels, y_pred) * len(test_labels), \" = \",  \n",
    "accuracy_score(test_labels, y_pred) * 100, \"%   \", \"INCORRECT: \", \n",
    "len(test_labels) - accuracy_score(test_labels, y_pred)*len(test_labels), \" = \",  \n",
    "100 - accuracy_score(test_labels, y_pred) * 100, \"%\")) #print out results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
